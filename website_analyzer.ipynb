{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNEcaB+1yOsi3NEUtipGue",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizabethWaithera/Data_sciences_projects/blob/main/website_analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "class WebsiteAnalyzer:\n",
        "    def __init__(self, urls: List[str]):\n",
        "        self.urls = urls\n",
        "        self.design_patterns = {}\n",
        "        self.color_scheme = {}\n",
        "        self.layout_patterns = {}\n",
        "        self.component_library = {}\n",
        "\n",
        "    def fetch_website_content(self, url: str) -> tuple:\n",
        "        \"\"\"Fetches HTML and CSS content from a URL\"\"\"\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        }\n",
        "        try:\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Get HTML content\n",
        "            html_content = str(soup)\n",
        "\n",
        "            # Extract CSS content\n",
        "            css_content = \"\"\n",
        "            # From style tags\n",
        "            for style in soup.find_all('style'):\n",
        "                css_content += style.string or \"\"\n",
        "\n",
        "            # From external stylesheets\n",
        "            for link in soup.find_all('link', rel='stylesheet'):\n",
        "                if link.get('href'):\n",
        "                    try:\n",
        "                        css_url = link['href']\n",
        "                        if not css_url.startswith(('http://', 'https://')):\n",
        "                            base_url = '{uri.scheme}://{uri.netloc}'.format(\n",
        "                                uri=urlparse(url))\n",
        "                            css_url = base_url + css_url\n",
        "                        css_response = requests.get(css_url, headers=headers, timeout=5)\n",
        "                        if css_response.status_code == 200:\n",
        "                            css_content += css_response.text\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Could not fetch CSS from {css_url}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "            return html_content, css_content\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching content from {url}: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def analyze_structure(self, html_content: str) -> Dict:\n",
        "        \"\"\"Analyzes website structure and identifies key patterns\"\"\"\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        structure = {\n",
        "            'layout_type': self._detect_layout_type(soup),\n",
        "            'main_sections': self._identify_main_sections(soup),\n",
        "            'navigation_type': self._analyze_navigation(soup),\n",
        "            'grid_system': self._detect_grid_system(soup),\n",
        "            'responsive_elements': self._find_responsive_elements(soup)\n",
        "        }\n",
        "        return structure\n",
        "\n",
        "    def _detect_layout_type(self, soup) -> str:\n",
        "        \"\"\"Detect the main layout type used\"\"\"\n",
        "        if soup.find_all(class_=re.compile(r'grid|flex')):\n",
        "            return 'modern-flex-grid'\n",
        "        elif soup.find_all('table', recursive=False):\n",
        "            return 'table-based'\n",
        "        else:\n",
        "            return 'standard-flow'\n",
        "\n",
        "    def _identify_main_sections(self, soup) -> List[str]:\n",
        "        \"\"\"Identify main sections of the website\"\"\"\n",
        "        sections = []\n",
        "        for tag in ['header', 'nav', 'main', 'footer', 'aside']:\n",
        "            if soup.find(tag):\n",
        "                sections.append(tag)\n",
        "        return sections\n",
        "\n",
        "    def _analyze_navigation(self, soup) -> str:\n",
        "        \"\"\"Analyze navigation structure\"\"\"\n",
        "        nav = soup.find('nav')\n",
        "        if nav:\n",
        "            if nav.find('ul'):\n",
        "                return 'hierarchical'\n",
        "            elif nav.find_all(class_=re.compile(r'menu|navigation')):\n",
        "                return 'custom-menu'\n",
        "        return 'simple'\n",
        "\n",
        "    def _detect_grid_system(self, soup) -> str:\n",
        "        \"\"\"Detect if a grid system is being used\"\"\"\n",
        "        grid_classes = soup.find_all(class_=re.compile(r'grid|row|col'))\n",
        "        if grid_classes:\n",
        "            return 'grid-based'\n",
        "        return 'no-grid'\n",
        "\n",
        "    def _find_responsive_elements(self, soup) -> List[str]:\n",
        "        \"\"\"Find responsive design elements\"\"\"\n",
        "        responsive_elements = []\n",
        "        media_queries = soup.find_all(class_=re.compile(r'mobile|tablet|desktop|lg|md|sm|xs'))\n",
        "        if media_queries:\n",
        "            responsive_elements.append('media-queries')\n",
        "        if soup.find('meta', attrs={'name': 'viewport'}):\n",
        "            responsive_elements.append('viewport-meta')\n",
        "        return responsive_elements\n",
        "\n",
        "    def analyze_all_websites(self) -> Dict:\n",
        "        \"\"\"Analyzes all provided websites\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for url in self.urls:\n",
        "            print(f\"\\nAnalyzing {url}...\")\n",
        "            try:\n",
        "                html_content, css_content = self.fetch_website_content(url)\n",
        "\n",
        "                if html_content and css_content:\n",
        "                    results[url] = {\n",
        "                        'structure': self.analyze_structure(html_content),\n",
        "                        'design_system': self.extract_design_system(html_content, css_content)\n",
        "                    }\n",
        "                    print(f\"Successfully analyzed {url}\")\n",
        "                else:\n",
        "                    print(f\"Could not fetch content from {url}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error analyzing {url}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extract_design_system(self, html_content: str, css_content: str) -> Dict:\n",
        "        \"\"\"Extracts complete design system information\"\"\"\n",
        "        return {\n",
        "            'colors': self._extract_colors(css_content),\n",
        "            'typography': self._extract_typography(css_content),\n",
        "            'spacing': self._extract_spacing(css_content),\n",
        "            'components': self._extract_components(html_content)\n",
        "        }\n",
        "\n",
        "    def _extract_colors(self, css_content: str) -> Dict:\n",
        "        \"\"\"Extract color patterns from CSS\"\"\"\n",
        "        colors = {\n",
        "            'primary': set(),\n",
        "            'background': set(),\n",
        "            'text': set()\n",
        "        }\n",
        "\n",
        "        # Find all color values in CSS\n",
        "        color_pattern = r'#[0-9a-fA-F]{3,6}|rgb\\([^)]+\\)|rgba\\([^)]+\\)'\n",
        "        found_colors = re.findall(color_pattern, css_content)\n",
        "\n",
        "        for color in found_colors:\n",
        "            colors['primary'].add(color)\n",
        "\n",
        "        return {k: list(v) for k, v in colors.items()}  # Convert sets to lists\n",
        "\n",
        "    def _extract_typography(self, css_content: str) -> Dict:\n",
        "        \"\"\"Extract typography patterns\"\"\"\n",
        "        typography = {\n",
        "            'fonts': set(),\n",
        "            'sizes': set()\n",
        "        }\n",
        "\n",
        "        # Find font families\n",
        "        font_pattern = r'font-family:\\s*([^;}]+)'\n",
        "        fonts = re.findall(font_pattern, css_content)\n",
        "        typography['fonts'].update(fonts)\n",
        "\n",
        "        # Find font sizes\n",
        "        size_pattern = r'font-size:\\s*([^;}]+)'\n",
        "        sizes = re.findall(size_pattern, css_content)\n",
        "        typography['sizes'].update(sizes)\n",
        "\n",
        "        return {k: list(v) for k, v in typography.items()}\n",
        "\n",
        "    def _extract_spacing(self, css_content: str) -> Dict:\n",
        "        \"\"\"Extract spacing patterns\"\"\"\n",
        "        spacing = {\n",
        "            'margins': set(),\n",
        "            'paddings': set()\n",
        "        }\n",
        "\n",
        "        # Find margin and padding values\n",
        "        margin_pattern = r'margin:\\s*([^;}]+)'\n",
        "        padding_pattern = r'padding:\\s*([^;}]+)'\n",
        "\n",
        "        spacing['margins'].update(re.findall(margin_pattern, css_content))\n",
        "        spacing['paddings'].update(re.findall(padding_pattern, css_content))\n",
        "\n",
        "        return {k: list(v) for k, v in spacing.items()}\n",
        "\n",
        "    def _extract_components(self, html_content: str) -> List[str]:\n",
        "        \"\"\"Extract common components\"\"\"\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "        components = []\n",
        "\n",
        "        # Look for common component patterns\n",
        "        if soup.find('nav'): components.append('navigation')\n",
        "        if soup.find('footer'): components.append('footer')\n",
        "        if soup.find_all(class_=re.compile(r'button|btn')): components.append('buttons')\n",
        "        if soup.find_all(class_=re.compile(r'card')): components.append('cards')\n",
        "        if soup.find_all('form'): components.append('forms')\n",
        "\n",
        "        return components\n",
        "\n",
        "# Example usage:\n",
        "websites = [\"https://www.dior.com/en_ie\", \"https://eu.louisvuitton.com/eng-e1/homepage\"]\n",
        "analyzer = WebsiteAnalyzer(websites)\n",
        "results = analyzer.analyze_all_websites()\n",
        "\n",
        "# Print results nicely formatted\n",
        "for url, data in results.items():\n",
        "    print(f\"\\nAnalysis results for {url}:\")\n",
        "    print(\"\\nStructure:\")\n",
        "    for key, value in data['structure'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "    print(\"\\nDesign System:\")\n",
        "    for key, value in data['design_system'].items():\n",
        "        print(f\"  {key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSWvybdLELXC",
        "outputId": "43c647b2-4d39-4389-c4c4-9ec77532e8ee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzing https://www.dior.com/en_ie...\n",
            "Successfully analyzed https://www.dior.com/en_ie\n",
            "\n",
            "Analyzing https://eu.louisvuitton.com/eng-e1/homepage...\n",
            "Error fetching content from https://eu.louisvuitton.com/eng-e1/homepage: HTTPSConnectionPool(host='eu.louisvuitton.com', port=443): Read timed out. (read timeout=10)\n",
            "Could not fetch content from https://eu.louisvuitton.com/eng-e1/homepage\n",
            "\n",
            "Analysis results for https://www.dior.com/en_ie:\n",
            "\n",
            "Structure:\n",
            "  layout_type: modern-flex-grid\n",
            "  main_sections: ['header', 'nav', 'main', 'footer']\n",
            "  navigation_type: simple\n",
            "  grid_system: grid-based\n",
            "  responsive_elements: ['media-queries', 'viewport-meta']\n",
            "\n",
            "Design System:\n",
            "  colors: {'primary': ['rgb(0 0 0/70%)', 'rgb(0 0 0/40%)', 'rgba(51, 56, 60, 0.04)', '#f6f6f6', '#b3b3b3', 'rgba(35, 39, 42, 1)', '#303030', '#f3f3f3', 'rgba(0, 0, 0, 0.6)', '#acb2b4', 'rgba(0,0,0,.1)', 'rgba(177, 179, 180, 1)', '#5D676C', 'rgba(0,0,0,.05)', '#f2f2f4', '#f3f3f5', '#2ac212', 'rgb(0 0 0/50.2%)', '#acce', 'rgba(0,0,0,.5)', '#F8F8F8', '#666', '#03b2cb', 'rgba(0,0,0,.55)', '#000000', '#c53929', '#fff', '#F3F3F3', 'rgba(51, 56, 60, 0.2)', '#FFFFFF', 'rgba(0,0,0,0)', '#555', '#E5E5E5', '#33383c', '#7b8487', '#5d676c', '#1574c3', 'rgba(0, 0, 0, 0.23)', '#d70000', 'rgba(0,0,0,.7)', 'rgba(51, 56, 60, 0.08)', 'rgba(0, 0, 0, 0.54)', 'rgb(0 0 0/19%)', 'rgb(0 0 0/30%)', '#7B8487', '#cecece', '#000', '#f5f5f5', 'rgba(0, 0, 0, 0.04)', 'rgba(0,0,0,0.2)', 'rgb(0 0 0/23%)', 'rgba(51, 56, 60, 0.5)', 'rgba(0, 0, 0, 0.26)', '#989494', '#e5e5e5', '#eeeeee', '#d09c17', '#ACB2B4', 'rgb(0 0 0/60%)', '#33383C', '#D1D1D1', '#f8f8f8', 'rgba(0,0,0,0.12)', '#eee', 'rgba(0,0,0,.2)', '#C53929', 'rgba(0,0,0,0.14)', '#757575', 'rgba(0, 0, 0, 0.12)'], 'background': [], 'text': []}\n",
            "  typography: {'fonts': ['Hellix!important', 'Century Gothic Std', 'Pathway Gothic One', 'Century Gothic Std,ABCDiorIcons,arial,sans-serif', 'Hellix,ABCDiorIcons,arial,sans-serif', 'inherit', 'DinCondensedBold', 'Hellix,arial,sans-serif', 'DinCondensedBold,Noto Sans JP,Noto Sans TC,Noto Sans KR,Century Gothic Std,arial,sans-serif', 'Century Gothic Std,arial,sans-serif', 'Century Gothic Std Bold', 'Hellix', 'Atacama VAR,ABCDiorIcons,arial,sans-serif', 'Pathway Gothic One,arial,sans-serif', 'ABCDiorIcons', 'Century Gothic Std Bold,Noto Sans JP,Noto Sans SC,Noto Sans TC,Noto Sans KR'], 'sizes': ['.3125rem', '2rem', '0.75em', 'var(--typographylabel-mboldfont-size)', '.8125rem', '1rem', '.875rem!important', '.75rem', '.875rem', '.625rem', '0.75rem', '.75rem!important', 'inherit', '1rem!important', '14px!important', '1.5rem', '.6875rem', '20px!important', '14px', '1.25rem', '0', '0.875rem', '2.5rem', '1.375rem']}\n",
            "  spacing: {'margins': ['1.25rem 0', '0 .5rem', '1.5rem auto auto', '1rem 0 .75rem', '0px', '8px 0px 0px 0px', '0 0 0 1rem', '12px 0', '0 .75rem 0 0', '4.6875rem', '.25rem 0', '0 1em', '1.5rem 0', '0 1.25rem', '1.5rem 0 0', '0!important', 'auto', '0 var(--spacings-mediumS) 0 calc(var(--spacings-mediumS)*2)', '0 0 2rem', '0 .9375rem .625rem', '2px', '0 var(--spacings-mediumXs) 0 var(--spacings-mediumXs)', '0 0 2.5rem', '.75rem auto 0', '0 0 0 .5rem', '0 var(--spacings-smallL) 0 var(--spacings-mediumS)', '.5rem auto', '.25rem 0 0', '.5rem 0 0', '3rem 0 0', '1rem 0 0', '0 0 .75rem', 'var(--spacings-mediumS) var(--spacings-mediumXs) var(--spacings-smallL) var(--spacings-mediumS)', '0 0 1.5rem', '0 auto 1rem', '0 .75rem 0 1rem', '0 0 .25rem', 'auto 0 1rem', '3rem 0', '0 0 0 var(--spacings-mediumS)', '0 var(--spacings-mediumXs) 0 0', '2.1875rem 0 .9375rem', '2.5rem 0', '0 var(--spacings-mediumS)', '0 .25rem 0 0', '0 0 3rem', '2.5rem 0 0', '0 0 0 .25rem', '.3125rem 0', '0 auto', '.375rem 0 0', '-.0625rem', '0 0 0 1em', 'var(--spacings-smallXl) 0', '1.25rem', '0 0 1rem', '0 var(--spacings-smallL) 0 var(--spacings-smallL)', '.75rem 0 0', '.5rem', '1rem auto', '1.25rem auto', '0 0 0 1.5rem', '0', '2rem 0', '0 var(--spacings-mediumXs) 0 calc(var(--spacings-mediumS)*2)', '2rem 0 0', '0 .25rem', '2.5rem', '0 1.4375rem 1.0625rem 1.25rem', '0 1em 0 0', '0 .9375rem', 'var(--spacings-mediumS) var(--spacings-mediumXs) var(--spacings-smallL) calc(var(--spacings-mediumS)*2)'], 'paddings': ['0px', '2rem 1rem', '16px', '4rem 0', '.25rem .5rem 0', '1.5rem 1.25rem', '1rem', '1.5rem 0', '1.5rem 0 2.5rem', '1rem 1.09375rem', 'var(--spacings-mediumS) 0', '.75rem 15%', '0 8px', '0 var(--spacings-smallXxl)', '0 4px', '4px 0 5px', '.1875rem .5rem .1875rem .25rem!important', '1.5rem 0 4rem', '0 1rem', '0 0 .25rem', '3.75rem', '1.25rem 1.25rem .375rem 0', '0 1.5rem', 'calc(3rem/2)!important', '1.5rem', '0px 0px 8px 0px', '2.5rem 0 2.5rem 2.5rem', 'var(--spacings-smallXl) 0', '0 0 0 .75rem', '1.5rem 0 2rem', '12px 0px 24px 0px', '0 0 .5rem', '.125rem .3125rem', '6rem 0 0', '0 .125rem', '3rem 0 3rem 3rem', '1.5rem 0 0', 'var(--spacings-mediumM) var(--spacings-smallL) var(--spacings-smallL) var(--spacings-smallL)', '1rem 3rem', '.5rem 1.25rem', '.5rem 3rem 3rem', '0px 8px', '1.25rem .5rem .375rem 0', '9px', 'var(--spacings-smallL) var(--spacings-smallM)', 'var(--spacings-smallL) 0', '0 var(--spacings-mediumS)', '3rem', '16.5px 14px', '0 1.78125rem', '6px 16px', '.5rem', '.4375rem 0', '0 .9375rem', '.3125rem', '1.5rem 3rem 3rem', '0 .5rem', '2rem', '0 0 0 1rem', '.5rem 0', '0 .625rem 0 0', '1.16rem', 'calc(2.5rem/2)!important', '.5rem 0 0', 'var(--spacings-smallXl) 0 0 0', '.25rem 0 .5rem', '0 2rem', '4.0625rem 3.125rem', '5rem 0', '2rem 3rem', '12px', '2.25rem 1.25rem 0', 'calc(1.25rem/2)!important', '7.5rem 0 0', '.3125rem 1.25rem', '0', '0 .1875rem', '1.5rem 0 .25rem', '0 1.25rem 0 0', '4px 8px 8px', '12px 24px', '2rem 0 2rem 2rem', '32px', '1.5rem 0 5rem', 'var(--spacings-smallM)', '1.78125rem', '40px', 'var(--spacings-smallL)', '0 .75rem 0 0', '0 1.25rem', '0!important', '0 1rem .375rem', '.75rem', '.5625rem 0', '0 3rem', '1rem 1.5rem 1rem 1.25rem', '.25rem 0 0', '.1875rem .5rem', '0 0 0 .5rem', '.1875rem .25rem .1875rem .5rem!important', '0 1.5rem 2rem 1.25rem', '.75rem 0', 'var(--spacings-mediumS) var(--spacings-smallL)', '0 1rem .75rem', '1.25rem', '0 0 1rem', '.75rem 0 0', '5px 15px', '.625rem 2.5rem', '2.5rem', '.9375rem']}\n",
            "  components: ['navigation', 'footer', 'forms']\n"
          ]
        }
      ]
    }
  ]
}